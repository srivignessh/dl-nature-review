
[1] Krizhevsky et al 2012

A breakthrough that used convolutional nets to almost
halve the error rate for object recognition and precipitated the
rapid adoption of deep learning by the computer vision community.
        
[6] Hinton et al 2012

A joint paper from the major speech recognition labs
summarizing the breakthrough achieved with deep learning on the task
of phonetic classification for automatic speech recognition. This
was the first major industrial application of deep learning.

[17] Sutskever et al 2014

One of the 2014 papers showing how recurrent networks could
be trained to read a sentence in one language, produce
a semantic representation of its meaning, and generate a translation
in another language.

[28] Glorot et al 2011

This paper showed that supervised training of very deep
neural networks is much faster if the hidden layers are composed of
rectified linear units.

[32] Hinton et al 2006

Introduced a novel and effective way of training very
deep neural networks by pre-training one hidden layer
at a time using the unsupervised learning procedure
for restricted boltzmann machines.

[33] Bengio et al 2007

Demonstrated that the unsupervised pre-training method introduced in
[32] significantly improves performance on test data  and generalizes
the method to other unsupervised representation learning
techniques, such as auto-encoders.


[41] LeCun et al 90

The first paper on convolutional networks trained by backpropagation,
for the task of classifying low-resolution images of handwritten digits.

[42] LeCun et al 98

Overview paper on the principles of end-to-end training of modular systems
such as deep neural networks using gradient-based optimization.  It also
showed how neural networks (and in particular convolutional nets) can be
combined with search or inference mechanisms to model complex outputs that
are interdependent, such as sequences of characters associated with the
content of a document.

[71] Bengio et al 2001

Introduced neural language models which learn to convert
a word symbol into  a word vector or word embedding
composed of learned semantic features in order to
predict the next word in a sequence.

[79] Hochreiter & Schmidhuber 97

Introduced long short term memory recurrent networks,
which have become a crucial ingredient in recent advances with
recurrent networks because they are good at learning
long-range dependencies.

