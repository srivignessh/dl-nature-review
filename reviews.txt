Editor
------

INFORMAL RESPONSE:
On Wed, Mar 18, 2015 at 7:35 AM, Chouard, Tanguy <T.Chouard@nature.com> wrote:
Dear Yann, Yoshua and Geoff,

Your draft Review has now been seen by two of the three reviewers enlisted -- the third expert fell ill in the interval, but we're hoping to get his feedback in the coming days…

At this stage, I thought you may find helpful to get the feedback already at hand (appended and attached), and start cracking at revising your manuscript. Production is now breathing down our necks quite heavily and time is becoming very tight…

As we've all known, size will, eventually, be an issue. I have battled on a couple of fronts to get you guys some breathing space, and here's where we sit right now: The draft as it stands is quite longer (at 12 estimated pages) than the usual limit for Nature Reviews (6 to 8 pages). We have, however, been able to gain some space on the other two 'Machine Learning' Reviews, and so we've managed to negotiate a bit of extra space for your piece (one, perhaps even two extra pages, depending on weather…). The piece will, nonetheless, have to be condensed by 20-25%.

In doing so, we should endeavour to preserve both accessibility and topicality. Deep Learning being red hot as it is, it is hard for us to select which places to cut in priority, for most areas covered seem quite vibrant indeed (with most references dating from just the past couple of years). We are, therefore, discussing the issue further with our peer-reviewers...

% YB: reduced parts on kernel machines, keep core business of DL


At this point, however, I thought I should give you this update, so you may start thinking carefully about where you might prefer to cut from in priority.

More very soon!…

--with many thanks again for a truly exciting piece.

Tanguy

FORMAL RESPONSE:

Dear Yann,

Your Review entitled "Deep Learning" has now been seen by three reviewers, whose comments are appended. 

While the reviewers find your review both timely and interesting (as we certainly do!), they have raised concerns which should be addressed expeditiously before we can proceed further.

We therefore invite you to revise your manuscript, as fast as possible, taking their points into account and ensuring accessibility to our broad audience. 

As said a couple of days ago, we estimate that your review currently exceeds our limit for Review articles (8 pages) by four (4) pages (with 11.6 estimated pages effectively spilling onto a twelfth page). Again, I am hoping to be able to secure a bit of extra-space on this exceptional occasion, but the manuscript will, eventually, have to fit within nine, possibly ten (9-10) final pages of Nature in print, which, at this stage, implies a 20-25% cut [the uncertainty comes from concurrent processing of five other Review Articles to be co-published in our strictly paginated Supplement on 'Machine Intelligence']. 

In other words, we *may* have a bit of extra-breathing space in the final stages of production (very soon now), but we can't really count on this at the moment. In practical terms, of the 24 pages of the current draft document, 5 to 6 pages should go immediately. This can be achieved by attention to the following editorial points, as well as some additional comments obtained from the reviewers (as appended to each report below).

FORMAT: The title should be informative and accessible but must not contain more than 80 characters (including spaces) or punctuation. 

Reviews start with a 100-word maximum preface, which should set the stage and end with a summary sentence. Please note that the preface will also appear on PubMed and Medline, so it is important that it contains essential key words. 
% YB: done, reduced to less than 100 words

The preface is followed by the main text, which should, ideally, be less than 5000 words in total and organized into three main sections: a brief (two to three paragraphs) introduction, your review of the topic, and a forward-looking ending. 

Please ensure that your introductory paragraphs serve three purposes: to endow the reader with sufficient background knowledge to appreciate the review; explain why this is an exciting time for the field; and summarize the aspects that will be discussed, together with the main conclusions. 

The remainder of the main text should be organized by subheadings in order to guide the reader around the review. Each subheading must be brief (no more than 44 characters) yet informative, and must not contain punctuation; in addition, there should be no more than two levels of subheadings. Figures should be used to illustrate major points, tables to summarize data, and boxes to expand on technical or peripheral points (see further details below). 

The final section should not summarize the review, but should give your perspective on future work (new directions or outstanding problems, for example). 

Please note that the ratio of text to display items is important, because there needs to be enough text to wrap around the display items. For example, a manuscript consisting of 4500 words and four moderately sized figures works well, but a manuscript of 3000 words and six such figures would present problems in fitting your article onto the page. 

REFERENCES: Reference citations are limited to 100, all of which should be accepted or published work. You may cite review articles when covering background information, but please focus on the primary literature to back-up the main points in your review. Please include any other articles in this Insight that you may wish to cite in the reference list. We ask that you select the most significant 5-10% of the references in your list for highlighting, although you must observe the upper limit of 10%. 

References must be cited in the order they appear in the text, tables, figure legends, and, finally, boxes. Please use 'et al.' where there are six or more authors and supply the title for each reference. Citations to books have the name and city of publisher; citations to reports should include relevant accession numbers; and citations to websites should give the full URL. Highlighted references should be followed by a single sentence in bold describing the main result and its significance. 

DISPLAY ITEMS: We encourage the use of figures, tables and boxes and suggest inclusion of around four original display items to illustrate the text. Figures that take the form of annotated schematics and flow diagrams can be helpful in reviews. Complicated tables and multi-part figures are best avoided - most readers are not specialists, so display items must be simple and informative. In addition, boxes of 'contained' information can be used to describe subject matter that may be too technical for the general reader. Please bear in mind that the ratio of text to display items is important. We encourage display items to illustrate the text, but there must be sufficient text to wrap around the display items. For example, a manuscript consisting of 4000 words and four moderately sized figures works well, but a manuscript of 3000 words and six such figures would present problems in fitting your article onto the page. 

Boxes should occupy no more than half a page in Nature (less than 500 words, or 250 words with a small figure) and may include a figure. We discourage the use or adaptation of previously published figures. Please also note that we cannot reproduce images downloaded from the Internet unless permission is obtained from the copyright holder. All display items should have a concise title and, in addition, figures should have an informative legend (no more than 250 words) that describes the parts, symbols and colours. Display items are cited in the text as (Fig. 1, Table 1, Box 1) and so on. 

Our art department will redraw your figures in full colour, but it is important that you provide us with detailed and accurate versions of what you require. Line drawings, text in all figures, and data that you do not expect us to redraw (such as graphs) should be provided in an editable Adobe Illustrator .ai, EPS, PDF, Canvas, postscript or Powerpoint format. For any photographic images that are to be used in your figures, we also require electronic files (JPEG or TIF) at high resolution (300 dpi at print size). Colour images must be supplied in CMYK format. In addition, it would be a great help if images were supplied without any embedded, uneditable labelling (such as arrows, lines or text) over the images themselves. If, however, you need to supply images in this format, then please ensure that you also supply the original images without the labelling. 

SUBMISSION: Your text file must be submitted as a Word document. Any tables should be prepared using the Table menu. The document should be ordered as main text, tables, figure legends and then boxes. 

When submitting your revised manuscript, please:

* Include a point-by-point response to any issues raised by our referees and our editorial requests.

* State in a cover note the word counts for the preface, main text, boxes (if any) and legends; the number of references; and the estimated final size of each figure and table (when reduced to the minimum reasonable size).

* Check that the references are in the correct format and order and that the top 5-10% have been annotated. 

* Ensure the file is uploaded as a Word document. 

*If you are using or adapting any previously published figures, you must also submit your permission from the copyright holder. 


* Resubmit electronically using the following link:


LINK TO YANN'S HOMEPAGE WAS HERE.




***This URL links to your confidential home page, and therefore any associated information about manuscripts you may have submitted or be reviewing for us. If you wish to forward this email to colleagues, please delete the link to your homepage first** 




We look forward to receiving your revised manuscript and hope you will be able to submit it within ten (10) days. Please let us know immediately if the revision process is likely to take longer.

With best regards,

Tanguy


Tanguy Chouard, PhD
Senior Editor
Nature
------------------------------
------------------------------

Referee #1 (Remarks to the Author)
== Signed Review: Jay McClelland ==

This ms. reviews recent breakthroughs in Deep Learning in the context of the history of approaches to this problem that have been under development for 30 years.  The publication is timely because it has now become clear that Deep Learning has really produced advances over what can be achieved for a wide range of 'natural' cognitive tasks (object, character and speech recognition) and now an ever-wider range of cognitively interesting tasks including language understanding and machine translation.

As this is a review article, I approach it from the point of view of accessibility, clarity of presentation, and due credit to originators of ideas.  Overall the paper seems fairly approachable, though I would say that the choice of level of detail to present in the text could use a bit more consideration. 

The text starting 'the backpropagation procedure' on p 4 is the key case in point.  The mathematical ideas here, particularly their presentation in equation form, might derail some readers.  Do we need to know that the matrix of partial derivatives is called the Jacobian Matrix?  Do we need to see the expression for dE/dx = dy/dx dE/dy?  I think all of the symbols and definitions can be placed in figure 1.  
% YB: DONE. We have removed the equations and their mathematical explanation from the text.

Additional comments here:

- The naive reader sould be a touch confused by the reversal of order of z and y from the top right of the figure to the rest of the figure lower down.  I understand that in the top right x, y, and z are strictly generic, whereas they have specific roles as neural network values in the bottom panels.  It would be easy to avoid confusion here by using a, b, and c to refer to generic variables and using x, z, and y to refer to neural network variables.  Of course the neural network variables could be more memorable than x, y, and z -- z could be replaced by n for net input for example.
% YB: suggest to Nature staff to redo the chain-rule.pdf figure with y and z swapped for one another, to match the other figures.
% YB: alternatively, if redoing all the figures, use x for input h for hidden and y for output.

- The abstract discussion of modules could be related more explicitly to the layers in the neural network.  For example, the network on the bottom of the figure could be decribed explicitly as having three modules, where the role of x for the top module is played by the y's at h2, etc.
% YB: done, in the caption of the backprop figure-box 1.

- The interpretation of Fig 2 is far from obvious.  I assume that the second column of the figure is the first set of filters, and the third column is the result of pooling/downsampling the first set of filters, but it is not perfectly clear that the column are arranged so that the pooled result corresponds to the unpooled input.  Lines connecting layers appropriately would help (first col goes to all six first-layer arrays, each of these to just one downsampling array -- then I assume each of these goes to all of the arrays at the next later, which in turn each project to the corresponding down-sampled array in the 5th column.  The sixth column appears to be different -- is this convolutional?  Another improvement would be to label the columns 'first filter layer' first pooling layer, etc.
% YB: done, labels added in the figure and caption clarifying as requested the meaning of what is displayed.

A major issue that needs to be dealt with in revision is the handling of the material related to unsupervised learning.  The paper currently reflects the differing perspectives of the authors on this issue and a greater degree of integration is necessary to avoid the perception that the different parts were written independently and without cognizance of each other.  The story about unsupervised learning can and should be woven into the main narrative of the paper.  Giving credit to the wave of unsupervised learning work of the early part of the new century together with the subsequent breakthroughs in back propagation methods as currenrly described.  In this way the material on pages 15-16 will not seem so starkly out of place as it does at present.
% YB: I am not sure to understand the concern; we do talk about unsupervised pre-training in the section on 'overcoming the limitations of backprop'. What differing perspectives?

In addition to the above comments, I had several minor comments that I provide on an annotated copy of the ms.  These comments relate to literature that I think needs to be cited from early work in the 1980s on neural networks, as well as issues of rhetorical stance.  As written the article does not do justice to the reasons why some were not convinced to use neural networks -- these people had reasons that seemed to make sense, and should not simply be characterized as hostile or biased.  The better way to present the work is that it suggests that these reasons, however legitimate, might have stood in the way of progress, which is possible if these considerations are downweighted.  A more concilliatory tone to past researchers will also minimize future resentments, which will not serve the current authors well when the current approaches hit an inevitable plateau.
% full re-read with that in mind is needed; proposed to remove the kernel paragraph altogether
% YB: rephrased the sentence about "prejudice" to talk in positive terms about the appeal of convexity
Finally the paper has many typos and infelicities of writing and deserves a careful proof-reading by a person who is careful and graceful in the use of English definite and indefinite articles. I have provided a commented version of the ms that highlights several such issues, though far from all of the ones that need attention in the ms.
% 
% YB: added a mention of CIFAR as informally suggested
% YB: added the Scholkopf book citation
% YB: removed the details of the family tree example, following the manuscript note in Bernhard's pdf
% YB: corrected many typos from Bernhard's pdf, and made suggested changes


------------------------------
------------------------------

Referee #2 (Remarks to the Author)

This is a wonderful review of a hot field. It's fine as is, but I'll make some suggestions concerning presentation given that the piece is supposed to be for the broad audience of Nature.  

The review could be more accessible to a broad audience (and more
thought-provoking even to specialists) if reorganized around a set of
questions that have been used as objections against neural nets in the
past.  The questions below are addressed in the piece but usually as
the conclusion to a section.  The order could be reversed so that the
questions are the leads.

MACHINE LEARNING
1) should features be hand-designed vs. learned?
% YB: this is already discussed

2) do optimizations need to be convex?
The text talks about the local minimum problem, but never mentions the word "convex." This was and is still a big deal in machine learning. Surprisingly,  neural nets suggest that convex optimization is a distraction.
% YB: a brief discussion of convexity has been added

ARTIFICIAL INTELLIGENCE
3) should we imitate the brain or not?
Perhaps the authors have consciously tried to minimize discussion of
this issue, but it seems strange not to discuss it more.
Nonspecialists and newcomers are curious to know whether artificial
neural nets are really brain-like.  The authors give the partial
explanation that the ConvNet architecture is inspired by Hubel and
Wiesel, but don't discuss whether backpropagation is biologically
plausible.  Also, now that neurocomputing is back in fashion, some are criticizing "brain-like" as hype.  See the recent interview with Michael Jordan for example.  Maybe the authors could give their opinion about this.
http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts

% YB: this is an interesting topic and we could talk about it, but we have been
% asked to cut 1/4th of the paper, so adding more material seems difficult

4) symbolic or connectionist?
This was a huge debate in the old days, and the review shows that neural nets are better at symbolic computing than anyone ever suspected.
% YB: that question is already covered

5)  probabilistic or deterministic?
People have argued that probabilistic representations are crucial for
machine learning and AI.  The innards of backprop-trained neural nets
are fully deterministic.  Isn't that surprising?
% backprop-trained neural nets (a) can compute conditional probabilities (e.g. for machine translation)
% and (b) can easily incorporate "injected noise", as in recent work with Variational Auto-Encoders cited
% at the end (Kingma et al NIPS 2014)

6)  modular or unstructured?
Minsky criticized connectionists as naively hoping that intelligence would emerge spontaneously from unstructured networks.  In reality, modern neural nets do have modular structure. A ConvNet has the convolution and pooling layers.  LSTM is another module for recurrent nets. And ConvNets are being used as modules in larger systems.

A few other thoughts:

When the authors compare deep learning with the rest of machine learning, they emphasize support vector machines.  What about Markov random fields and other graphical models?  These terms have been and still are a big deal, yet they don't even appear in the text. Why have they been pushed aside by neural nets?
% YB: graphical models are very present in the unsupervised side of deep learning
% but much more space would be needed to give justice to that aspect.
% Two sentences were added in the middle of this paragraph to clarify this:
"This review has focused on supervised learning because of its recent
achievements and we have ignored much interesting work on unsupervised
learning procedures for deep neural
networks~\citep{Salakhutdinov2009-small,Hinton95,QuocLe-ICML2012,VincentPLarochelleH2008-small,koray-nips-10,gregor-icml-10,ranzato-pami,Bengio-et-al-ICML-2014,Kingma-et-al-NIPS2014}.
These papers often introduce a probabilistic view or the injection of randomness in the
neural network computation, to allow neural networks to {\em generate} examples that are
similar to training examples but generalize them.
Unsupervised learning is important because most data have not been hand-labeled by humans, and just like
children are not told what they should have done at each instant, we
need learning machines that can learn from mostly unlabeled examples."


On a related note, the authors choose to emphasize the divide between
% YB: we have rephrased in a few places to make the tone less confrontational
deep learning and the rest of machine learning. One could also stress
the similarities.  A support vector machine is similar to a two-layer
perceptron.  
% YB: indeed, except that its first layer is fixed (the feature space view)
% or parametrized directly by the training examples themselves (the kernel function view).
% Again we would like to discuss that at greater length but space is heavily limited.
Markov random fields and graphical models are similar to
ConvNets and neural nets. Given the similarities, it seems especially strange that neural nets were rejected by the mainstream, and perhaps the authors could comment on the ideological reasons for that.

ConvNets have emerged victorious for visual object recognition, both
in academic research and in industry.  From the review it's less clear
whether recurrent nets have reached the same level of success. For
example, do these nets outperform conventional language models based
on quantitative metrics like BLEU?  Are recurrent nets used by
industry in commercial systems?
% YB: you are right that although there is enormous enthusiasm 
% with recurrent nets, the industrial adoption in products is less well known;
% we informally know of several products (e.g. for speech recognition)
% based on recurrent nets, but convolutional nets clearly have a head start.

It might be a good idea to be more cautious in declaring victory regarding the symbolic-connectionist debate, to avoid antagonizing the symbolic types.  Though I can see the appeal of kicking them while they are down :)
% Geoff, say something

Mnih et al. has appeared in Nature, so perhaps that should be cited rather than the tech report.
% YB: Done  
Tesauro's work on TD-gammon could be cited as an important precedent.
% YB: Done

Another bit of history: the earlier demonstrations that GPU implementations of neural nets could beat other machine learning techniques on the MNIST dataset were perhaps an important precursor to the ImageNet-winning ConvNet.
% YB: Schmidhuber?

Referee #3 (Remarks to the Author):

This is a timely review of a highly dynamic subfield of machine learning research, written by the luminaries of that subfield. I have read it with great interest and I believe that quite a few of Nature's readers would do so as well, now that interest in machine learning is greatly increasing in various fields of science and technology. The paper does use some jargon that not everybody knows, but it is overall well written, not too mathematical, and reasonably easy to understand.

There are two things that bothered me a little, but they are issues of personal taste, so I would not insist on those being changed in any way. 

The first one is that the article comes across as every so slightly "whiny" (this might not be the right English term) - it sounds like the authors feel upset that their methods did not get the recognition they deserved. When I entered the field, (multi-layer) neural networks were actually the standard method on many tasks - so I think that impression probably only refers to the local minimum in between the current waves and the wave in the early nineties. Maybe it need not be discussed in as much detail..
% YB: the tone has been changed to be less confrontational in a few places

The second one is that many of the impressive features and successes of deep learning applications, as described in the paper, I would really view more generally as successes of applying high capacity models to huge datasets - i.e., they are the result of taking a rich and well-designed class of models and training it on a huge training set. The current deep learning hype may wane again one day, but whatever we will be using next will probably also involve data-driven rich models. To me, that is the underlying long term trend - but again, this is my personal opinion, and it is fine if the review expresses the subjective point of view of the authors.
% YB: my opinion is different: yes high capacity flexible models trained on lots of data is an important ingredient,
% but by far it is insufficient. Consider for example the fate of decision trees. They can be trained quickly,
% they can efficiently handle very high capacity much better than neural nets, but they don't generalize well.
% Deep learning is motivated by ideas about {\em non-local} generalization that is possible thanks to the
% compositional effects arising out of (a) distributed representations (composition of features) and (b) depth
% (composition of layers).
%


Finally, some comments on details:

Section 3: I was surprized you're not mentioning the work of Bruna.
% YB: who is Bruna?

page 3: the linear support vector machine is much older and was developed by Vapnik and Chervonenkis
% YB: thanks, added the 1964 reference

page 3: kernel methods: I think Boser et al. is a good reference for SVMs, but for Kernel Methods in general (including the general kernel trick, and the fact that kernels can be defined on non-vectorial data, etc.), it would be better to cite the "Learning with Kernels" book, or the Cristianini book, or - if you prefer original articles - the kernel PCA paper.
% YB: done, we are citing the "Learning with Kernels" book.

page 12: "Thanks to advances..." - it would be interesting to know which advances you mean.
% YB: we added references for both the architecture advances and the training advances


ADDITIONAL COMMENTS FROM REVIEWER #3 REGARDING CUTS:

I was asked for suggestions which parts could be shortened. Here are some thoughts:
- I think section 5 is a bit tedious. Some of it will be lost on the readers. The points made in the last paragraph are very good though and should certainly be made somewhere. The section is also somewhat oddly placed in between the sections on image understanding and language processing.
% YB: Section 5 has been reduced

- section 9 is interesting, but if space is tight you might be able to remove it and salvage some material by moving it into section 12.

- section 11 is very interesting, but I think it could be shortened by 20% without losing much.



***END***


