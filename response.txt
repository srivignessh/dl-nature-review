
We have made the following main changes:

- reduced the preface to less than 100 words
- followed most of the suggestions from the reviewers
- reduced the text considerably, especially in the more technical parts
  (but still 3/4 pages over the target of 8 pages)
- reduced the number of citations from 139 to 105 (target was 100)

Regarding the reviews, please find more detailed comments below.  

---------

From Editor:
-----------

> In doing so, we should endeavour to preserve both accessibility and topicality. Deep Learning being red hot as it is, it is hard for us to select which places to cut in priority, for most areas covered seem quite vibrant indeed (with most references dating from just the past couple of years). We are, therefore, discussing the issue further with our peer-reviewers...

We have reduced parts on kernel machines and shallow learners, to keep our core business of deep learning.

From Referee 1:
--------------

> The text starting 'the backpropagation procedure' on p 4 is the key case in point.  The mathematical ideas here, particularly their presentation in equation form, might derail some readers.  Do we need to know that the matrix of partial derivatives is called the Jacobian Matrix?  Do we need to see the expression for dE/dx = dy/dx dE/dy?  I think all of the symbols and definitions can be placed in figure 1.  

Done. We have removed the equations and their mathematical explanation from the text.

> - The naive reader sould be a touch confused by the reversal of order of z and y from the top right of the figure to the rest of the figure lower down.  I understand that in the top right x, y, and z are strictly generic, whereas they have specific roles as neural network values in the bottom panels.  It would be easy to avoid confusion here by using a, b, and c to refer to generic variables and using x, z, and y to refer to neural network variables.  Of course the neural network variables could be more memorable than x, y, and z -- z could be replaced by n for net input for example.

We suggest that the Nature staff redoes the chain-rule.pdf figure with y and z swapped for one another, to match the other figures. Alternatively, if redoing all the figures, use x for input h for hidden and y for output.

> - The abstract discussion of modules could be related more explicitly to the layers in the neural network.  For example, the network on the bottom of the figure could be decribed explicitly as having three modules, where the role of x for the top module is played by the y's at h2, etc.

Done, in the caption of the backprop figure-box 1.

> - The interpretation of Fig 2 is far from obvious.  I assume that the second column of the figure is the first set of filters, and the third column is the result of pooling/downsampling the first set of filters, but it is not perfectly clear that the column are arranged so that the pooled result corresponds to the unpooled input.  Lines connecting layers appropriately would help (first col goes to all six first-layer arrays, each of these to just one downsampling array -- then I assume each of these goes to all of the arrays at the next later, which in turn each project to the corresponding down-sampled array in the 5th column.  The sixth column appears to be different -- is this convolutional?  Another improvement would be to label the columns 'first filter layer' first pooling layer, etc.

Done, labels added in the figure and caption clarifying as requested the meaning of what is displayed.

> A major issue that needs to be dealt with in revision is the handling of the material related to unsupervised learning.  The paper currently reflects the differing perspectives of the authors on this issue and a greater degree of integration is necessary to avoid the perception that the different parts were written independently and without cognizance of each other.  The story about unsupervised learning can and should be woven into the main narrative of the paper.  Giving credit to the wave of unsupervised learning work of the early part of the new century together with the subsequent breakthroughs in back propagation methods as currenrly described.  In this way the material on pages 15-16 will not seem so starkly out of place as it does at present.

We are not sure to understand the concern; we do talk about unsupervised pre-training in the section on 'overcoming the limitations of backprop'. What differing perspectives?

> In addition to the above comments, I had several minor comments that I provide on an annotated copy of the ms.  These comments relate to literature that I think needs to be cited from early work in the 1980s on neural networks, as well as issues of rhetorical stance.  As written the article does not do justice to the reasons why some were not convinced to use neural networks -- these people had reasons that seemed to make sense, and should not simply be characterized as hostile or biased.  The better way to present the work is that it suggests that these reasons, however legitimate, might have stood in the way of progress, which is possible if these considerations are downweighted.  A more concilliatory tone to past researchers will also minimize future resentments, which will not serve the current authors well when the current approaches hit an inevitable plateau.

We have changed the tone in many places to make it sound more positive.
We rephrased the sentence about "prejudice" to talk in positive terms about the appeal of convexity

> Finally the paper has many typos and infelicities of writing and deserves a careful proof-reading by a person who is careful and graceful in the use of English definite and indefinite articles. I have provided a commented version of the ms that highlights several such issues, though far from all of the ones that need attention in the ms.

We have made a lot of changes in the form and English, to clarify and remove errors.
We have made the other changes suggested by the referee, however, some suggestions
had to be dropped in order to reduce the size of the manuscript and the number of citations.


From Referee 2:
--------------

We have not re-organized the text around the suggested questions, but we have
made sure to address most of them.

> 1) should features be hand-designed vs. learned?

This is central in the paper and we argue about it.

> 2) do optimizations need to be convex?
> The text talks about the local minimum problem, but never mentions the word "convex." This was and is still a big deal in machine learning. Surprisingly,  neural nets suggest that convex optimization is a distraction.

A brief discussion of convexity and saddle points vs local minima has been added.

> 3) should we imitate the brain or not?

This is an interesting topic and we could talk more about it, but we have been
asked to cut 1/4th of the paper, so adding more material seems difficult. There
is a discussion of the inspiration from the visual cortex, for ConvNets.

> 4) symbolic or connectionist?

That question is covered.

> 5)  probabilistic or deterministic?
> People have argued that probabilistic representations are crucial for
> machine learning and AI.  The innards of backprop-trained neural nets
> are fully deterministic.  Isn't that surprising?

Backprop-trained neural nets (a) can compute conditional probabilities (e.g. for machine translation)
and (b) can easily incorporate "injected noise", as in recent work with Variational Auto-Encoders cited
at the end (Kingma et al NIPS 2014). We have not included an explicit discussion about this, though.

> 6)  modular or unstructured?
> Minsky criticized connectionists as naively hoping that intelligence would emerge spontaneously from unstructured networks.  In reality, modern neural nets do have modular structure. A ConvNet has the convolution and pooling layers.  LSTM is another module for recurrent nets. And ConvNets are being used as modules in larger systems.

ML theory clearly states that some form of prior (what you call structure) is necessary, so it's
not a black and white question. We believe that it is more valuable to uncover general-purpose
priors that cover a lot of tasks, and convolutions or LSTMs are of that kind. As the amount
of available training increases, this helps the most the more generic types of structure imposed
on the neural network.

> When the authors compare deep learning with the rest of machine learning, they emphasize support vector machines.  What about Markov random fields and other graphical models?  These terms have been and still are a big deal, yet they don't even appear in the text. Why have they been pushed aside by neural nets?

The space devoted to kernel machines has been mostly removed, in the interest of the requested 
length cuts.
Graphical models are very present in the unsupervised side of deep learning
but much more space would be needed to give justice to that aspect.
Two sentences were added in the middle of this paragraph to clarify this:
"This review has focused on supervised learning because of its recent
achievements and we have ignored much interesting work on unsupervised
learning procedures for deep neural
networks~\citep{Salakhutdinov2009-small,Hinton95,QuocLe-ICML2012,VincentPLarochelleH2008-small,koray-nips-10,gregor-icml-10,ranzato-pami,Bengio-et-al-ICML-2014,Kingma-et-al-NIPS2014}.
These papers often introduce a probabilistic view or the injection of randomness in the
neural network computation, to allow neural networks to {\em generate} examples that are
similar to training examples but generalize them.
Unsupervised learning is important because most data have not been hand-labeled by humans, and just like
children are not told what they should have done at each instant, we
need learning machines that can learn from mostly unlabeled examples."


> On a related note, the authors choose to emphasize the divide between
> deep learning and the rest of machine learning. One could also stress

We have rephrased in a few places to make the tone less confrontational

> the similarities.  A support vector machine is similar to a two-layer
> perceptron.  

Indeed, except that its first layer is fixed (the feature space view)
or parametrized directly by the training examples themselves (the kernel function view).
Again we would like to discuss that at greater length but space is heavily limited.

> ConvNets have emerged victorious for visual object recognition, both
> in academic research and in industry.  From the review it's less clear
> whether recurrent nets have reached the same level of success. For
> example, do these nets outperform conventional language models based
> on quantitative metrics like BLEU?  Are recurrent nets used by
> industry in commercial systems?

You are right that although there is enormous enthusiasm 
with recurrent nets, the industrial adoption in products is less well known;
we informally know of several products (e.g. for speech recognition)
based on recurrent nets, but convolutional nets clearly have a head start
(and of course it's not a competition, they make sense in different contexts
and have been combined, e.g., in the recent work on caption generation from images).

> Mnih et al. has appeared in Nature, so perhaps that should be cited rather than the tech report.

Done  

> Tesauro's work on TD-gammon could be cited as an important precedent.

Done

> Another bit of history: the earlier demonstrations that GPU implementations of neural nets could beat other machine learning techniques on the MNIST dataset were perhaps an important precursor to the ImageNet-winning ConvNet.

Yes, cited.

From Referee 3:
--------------

> The first one is that the article comes across as every so slightly "whiny" (this might not be the right English term) - it sounds like the authors feel upset that their methods did not get the recognition they deserved. When I entered the field, (multi-layer) neural networks were actually the standard method on many tasks - so I think that impression probably only refers to the local minimum in between the current waves and the wave in the early nineties. Maybe it need not be discussed in as much detail..

The tone has been changed to be less confrontational.

> The second one is that many of the impressive features and successes of deep learning applications, as described in the paper, I would really view more generally as successes of applying high capacity models to huge datasets - i.e., they are the result of taking a rich and well-designed class of models and training it on a huge training set. The current deep learning hype may wane again one day, but whatever we will be using next will probably also involve data-driven rich models. To me, that is the underlying long term trend - but again, this is my personal opinion, and it is fine if the review expresses the subjective point of view of the authors.

Ou opinion is different: yes high capacity flexible models trained on lots of data is an important ingredient,
but by far it is insufficient. Consider for example the fate of decision trees. They can be trained quickly,
they can efficiently handle very high capacity much better than neural nets, but they don't generalize well.
Deep learning is motivated by ideas about {\em non-local} generalization that is possible thanks to the
compositional effects arising out of (a) distributed representations (composition of features) and (b) depth
(composition of layers).

> Section 3: I was surprized you're not mentioning the work of Bruna.

Which Bruna do you refer to?

> page 3: the linear support vector machine is much older and was developed by Vapnik and Chervonenkis

We wanted to add the 1964 reference but we have been asked to cut citations, so it's going to
be up to the editor to decide.

> page 3: kernel methods: I think Boser et al. is a good reference for SVMs, but for Kernel Methods in general (including the general kernel trick, and the fact that kernels can be defined on non-vectorial data, etc.), it would be better to cite the "Learning with Kernels" book, or the Cristianini book, or - if you prefer original articles - the kernel PCA paper.

Done, we are citing the "Learning with Kernels" book.

> page 12: "Thanks to advances..." - it would be interesting to know which advances you mean.

We added references for both the architecture advances and the training advances

> - I think section 5 is a bit tedious. Some of it will be lost on the readers. The points made in the last paragraph are very good though and should certainly be made somewhere. The section is also somewhat oddly placed in between the sections on image understanding and language processing.

The old Section 5 has been reduced (note that section numbers have changed to due reorganizations
to cut pieces out).



